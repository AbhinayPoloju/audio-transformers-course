# Audio generation with a pipeline

Audio generation encompasses a versatile set of tasks that involve producing an audio output. The tasks 
that we will look into here are speech generation (aka Text-to-speech task) and music generation. In text-to-speech, a 
model transforms a piece of text into lifelike spoken language sound, opening the door to applications such as virtual assistants, 
accessibility tools for the visually impaired, and personalized audiobooks. 
On the other hand, music generation can enable creative expression, and finds its use mostly in entertainment and game 
development industries. 

In ðŸ¤— Transformers, you'll find a pipeline that covers both of these tasks. This pipeline is called `"text-to-audio"`, 
but for convenience, it also has a `"text-to-speech"` alias. Here we'll use both, and you are free to pick whichever 
seems more descriptive for your task. 

Let's explore how you can use this pipeline to start generating audio narration for texts, and music with just a few lines of code.

This pipeline is new to ðŸ¤— Transformers, thus you'll need to install the library from the source: 

```bash
pip install git+https://github.com/huggingface/transformers.git
```

## Generating speech

Let's begin by exploring text-to-speech generation. First, just as it was the case with audio classification and automatic 
speech recognition, we'll need to define the pipeline. We'll use the `suno/bark-small` model with this pipeline:

```python
from transformers import pipeline

pipe = pipeline("text-to-speech", model="suno/bark-small")
```

The next step is as simple as passing some text through the pipeline. All the preprocessing will be done for us under the hood: 

```python
text = "Ladybugs have had important roles in culture and religion, being associated with luck, love, fertility and prophecy. "
output = pipe(text)
```

In a notebook, we can use the following code snippet to listen to the result: 

```python
from IPython.display import Audio

Audio(output["audio"], rate=output["sampling_rate"])
```

The model that we're using with the pipeline, Bark, is actually multilingual, so we can easily substitute the initial 
text with a text in, say, French, and use the pipeline in the exact same way. It will pick up on the language all by itself:

```python
fr_text = "Contrairement Ã  une idÃ©e rÃ©pandue, le nombre de points sur les Ã©lytres d'une coccinelle ne correspond pas Ã  son Ã¢ge, ni en nombre d'annÃ©es, ni en nombre de mois. "
output = pipe(fr_text)
Audio(output["audio"], rate=output["sampling_rate"])
```

Not only is this model multilingual, it can also generate audio with non-verbal communications and singing. Here's how 
you can make it sing: 

```python
song = "â™ª In the jungle, the mighty jungle, the ladybug was seen. â™ª "
output = pipe(song)
Audio(output["audio"], rate=output["sampling_rate"])
```

We'll dive deeper into Bark specifics in the later unit dedicated to Text-to-speech, and will also show how you can use 
other models for this task. Now, let's generate some music!

## Generating music

Just as before, we'll begin by instantiating a pipeline. For music generation, we'll take the pretrained `facebook/musicgen-small` 
checkpoint.  

```python
music_pipe = pipeline("text-to-audio", model="facebook/musicgen-small")
```

Let's create a text description of the music we'd like to generate:

```python
text = "90s rock song with electric guitar and heavy drums"
```

For best results, we'll specify some additional music generation parameters to pass to `musicgen`. These are model-, and 
not pipeline-specific. 

- `do_sample` introduces some variability and a bit of randomness to improve the "creativeness" of the output
- `max_new_tokens` controls the length of the generated output
- higher `guidance_scale` encourages the model to generate samples more closely linked to the text prompt (at the expense of the audio quality). Guidance scale of 3 is a recommended default.  

```python
forward_params = {"do_sample": True, "max_new_tokens": 512, "guidance_scale": 3}

output = music_pipe(text, forward_params=forward_params)
Audio(output["audio"][0], rate=32000)
```

Note: the sampling rate value for music generation comes from the configuration of the model.
