# Preprocessing audio dataset

Loading a dataset with ðŸ¤— Datasets is just half of the fun. It is now time to pre-process the data and make it ready for
model training or inference. In this section, you'll learn how to perform three stages of preparing audio data for training:

* Resampling the audio data
* Pre-processing audio data
* Filtering data

## Resampling the Audio Data

The `load_dataset` function downloads audio samples with the sampling rate that they were published with. This is not
always the sampling rate expected by a model you plan to train. If there's a discrepancy between the sampling rates,
you can resample the audio to the model's expected sampling rate.

Most of the available pretrained models have been pretrained on audio datasets at a sampling rate of 16 kHz.
When we were exploring MINDS-14 dataset, you may have noticed that it is sampled at 8 kHz, which means we will liekly need
to upsample it, if we want to use it for fine-tuning a model.

To do so, use ðŸ¤— Datasets' `cast_column` method. This operation does not change the audio in-place, but rather signals
to datasets to resample the audio samples on the fly when they are loaded. The following code cell will set the sampling
rate to 16kHz:

```py
>>> from datasets import Audio

>>> minds = minds.cast_column("audio", Audio(sampling_rate=16_000))
```

Re-load the first audio example in the MINDS-14 dataset, and check that it has been resampled to the desired `sampling rate`:

```py
>>> minds[0]
{'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-AU~PAY_BILL/response_4.wav',
 'audio': {'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-AU~PAY_BILL/response_4.wav',
  'array': array([2.0634243e-05, 1.9437837e-04, 2.2419340e-04, ..., 9.3852862e-04,
         1.1302452e-03, 7.1531429e-04], dtype=float32),
  'sampling_rate': 16000},
 'transcription': 'I would like to pay my electricity bill using my card can you please assist',
 'intent_class': 13}
```

You may notice that the array values are now also different. This is because we've now got twice the amplitude value for
every one that we had before.

## Pre-processing audio data

<!--  TODO: it is unclear what kind of preprocessing happens under the hood. For a course it'd be great to explain it here -->
One of the most challenging aspects of working with audio datasets is preparing the data in the right format for model
training. Using ðŸ¤— Datasets' map method, you can write a function to pre-process a single sample of the dataset, and
then apply it to every sample without any code changes.

First, let's load a processor object from ðŸ¤— Transformers. This processor pre-processes the audio to input features and
tokenises the target text to labels. The AutoProcessor class is used to load a processor from a given model checkpoint.
In the example, we load the processor from OpenAI's Whisper medium.en checkpoint, but you can change this to any model
identifier on the Hugging Face Hub:

<!--  TODO re-write the preprocessing part -->
```py
>>> from transformers import AutoProcessor

>>> processor = AutoProcessor.from_pretrained("anton-l/xtreme_s_xlsr_300m_minds14")
```

Now we can write a function that takes a single training sample and passes it through the processor to prepare it for our model.
We'll also compute the input length of each audio sample, information that we'll need for the next data preparation step:

```py
>>> def prepare_dataset(batch):
...     audio = batch["audio"]
...     batch = processor(audio["array"], sampling_rate=audio["sampling_rate"], text=batch["text"])

...     batch["input_length"] = len(audio["array"]) / audio["sampling_rate"]
...     return batch
```

We can apply the data preparation function to all of our training examples using ðŸ¤— Datasets' map method. Here, we also
remove the text and audio columns, since we have pre-processed the audio to input features and tokenised the text to labels:

```py
minds = minds.map(prepare_dataset, remove_columns=minds.column_names)
```

## Filtering audio data

<!--  It's probably worth elaborating on reasons one may need to filter  -->
Prior to training, we might have a heuristic for filtering the training data. For instance, we might want to filter any
audio samples longer than 30s to prevent truncating the audio samples or risking out-of-memory errors.
We can do this in much the same way that we prepared the data for our model in the previous step.

We start by writing a function that indicates which samples to keep and which to discard. This function, `is_audio_length_in_range`,
returns a boolean mask where a value is: `True` if a sample is shorter than 30s, and `False` if it is longer than 30s.

```py
>>> MAX_DURATION_IN_SECONDS = 30.0

>>> def is_audio_length_in_range(input_length):
...     return input_length < MAX_DURATION_IN_SECONDS
```

We can apply this filtering function to all of our training examples using ðŸ¤— Datasets' filter method, keeping all
samples that are shorter than 30s (True) and discarding those that are longer (False):

```py
>>> gigaspeech["train"] = gigaspeech["train"].filter(is_audio_length_in_range, input_columns=["input_length"])
```

And with that, we have the GigaSpeech dataset fully prepared for our model!
These are only the fundamental data preparation steps, and, of course, custom data may require more complex preprocessing.
However, you can extend the function `prepare_dataset` to perform much more involved operations, such as data augmentation,
voice activity detection or noise reduction. With ðŸ¤— Datasets, if you can write it as a Python function, you can apply
it to your dataset!

