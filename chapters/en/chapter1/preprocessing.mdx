# Preprocessing an audio dataset

Loading a dataset with ðŸ¤— Datasets is just half of the fun. If you plan to use it either for training a model, or for running
inference, you will need to pre-process the data first. In general, this will involve the following steps:

* Resampling the audio data
* Filtering data
* Converting audio data to model's expected input


## Resampling the Audio Data

The `load_dataset` function downloads audio samples with the sampling rate that they were published with. This is not
always the sampling rate expected by a model you plan to train, or use for inference. If there's a discrepancy between
the sampling rates, you can resample the audio to the model's expected sampling rate.

Most of the available pretrained models have been pretrained on audio datasets at a sampling rate of 16 kHz.
When we explored MINDS-14 dataset, you may have noticed that it is sampled at 8 kHz, which means we will likely need
to upsample it.

<!--  TODO: @hollance please add an explanation of what happens when we upsample-->

To do so, use ðŸ¤— Datasets' `cast_column` method. This operation does not change the audio in-place, but rather signals
to datasets to resample the audio samples on the fly when they are loaded. The following code cell will set the sampling
rate to 16kHz:

```py
>>> from datasets import Audio

>>> minds = minds.cast_column("audio", Audio(sampling_rate=16_000))
```

Re-load the first audio example in the MINDS-14 dataset, and check that it has been resampled to the desired `sampling rate`:

```py
>>> minds[0]
{'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-AU~PAY_BILL/response_4.wav',
 'audio': {'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-AU~PAY_BILL/response_4.wav',
  'array': array([2.0634243e-05, 1.9437837e-04, 2.2419340e-04, ..., 9.3852862e-04,
         1.1302452e-03, 7.1531429e-04], dtype=float32),
  'sampling_rate': 16000},
 'transcription': 'I would like to pay my electricity bill using my card can you please assist',
 'intent_class': 13}
```

You may notice that the array values are now also different. This is because we've now got twice the number of amplitude values for
every one that we had before.

## Filtering audio data

You may need to filter the data based on some criteria. One of the common cases involves limiting the audio examples to a
certain duration. For instance, we might want to filter out any examples longer than 30s to prevent out-of-memory errors
when training a model.

We can do this by using the  ðŸ¤— Datasets' `filter` method and passing a function with filtering logic to it. Let's start by writing a
function that indicates which examples to keep and which to discard. This function, `is_audio_length_in_range`,
returns a boolean mask where a value is `True` if a sample is shorter than 20s, and `False` if it is longer than 20s.

```py
>>> MAX_DURATION_IN_SECONDS = 20.0

>>> def is_audio_length_in_range(input_length):
...     return input_length < MAX_DURATION_IN_SECONDS
```

The filtering function can be applied to a dataset's column but we do not have a column with audio track duration in this
dataset. However, we can create one, filter based on the values in that column, and then remove it.

```py
>>> # use librosa to get example's duration from the audio file
>>> new_column = [librosa.get_duration(filename=x) for x in minds["path"]]
>>> minds = minds.add_column("duration", new_column)

>>> # use ðŸ¤— Datasets' `filter` method to apply the filtering function
>>> minds = minds.filter(is_audio_length_in_range, input_columns=["duration"])

>>> # remove the temporary helper column
>>> minds = minds.remove_columns(["duration"])
>>> minds
Dataset({
    features: ['path', 'audio', 'transcription', 'intent_class'],
    num_rows: 624
})
```

We can verify that dataset has been filtered down from 654 examples to 624.

## Pre-processing audio data

One of the most challenging aspects of working with audio datasets is preparing the data in the right format for model
training. As you saw, the raw audio data comes as an array of values.

<!--  TODO: add a section explaining what feature extractor does exactly -->
<!--  Mention text tokenizers as well and link to NLP course for explanations -->

Luckily you do not have to worry about manually converting your raw audio data into the model's acceptable input format,
as ðŸ¤— Transformers offer model-specific processors that can take care of the necessary transformations, such as normalisation and log-mel.

To illustrate this, let's load a processor object from ðŸ¤— Transformers. Let's pretend that we plan to fine-tune the Wav2Vec2
model here, specifically the [`facebook/wav2vec2-base`](https://huggingface.co/facebook/wav2vec2-base) checkpoint from
the ðŸ¤— Hub. This means that we need to load the corresponding processor.

We can use the `AutoProcessor` class to identify and load a correct processor from a given model checkpoint:

```py
>>> from transformers import AutoProcessor

>>> processor = AutoProcessor.from_pretrained("facebook/wav2vec2-base")
```

Next you can write a function to pre-process a batch of examples of the dataset by passing them through the processor.

```py
>>> def prepare_dataset(batch):
...     audio = batch["audio"]
...     batch = processor(audio["array"], sampling_rate=audio["sampling_rate"], text=batch["transcription"], padding = True)
...     return batch
```

We can apply the data preparation function to all of our training examples using ðŸ¤— Datasets' map method:

```py
minds = minds.map(prepare_dataset)
```

And with that, we now could use the MINDS-14 dataset for training with a Wav2Vec2 model if we wanted to.

Here we have illustrated the fundamental data preparation steps, and, of course, custom data may require more complex preprocessing.
However, you can extend the function `prepare_dataset` to perform any sort of custom data transformations. With ðŸ¤— Datasets,
if you can write it as a Python function, you can apply it to your dataset!