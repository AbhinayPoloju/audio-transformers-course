# Introduction to audio data

By nature, a sound wave is a continuous signal, meaning it contains an infinite number of signal values in a given time.
This poses problems for digital devices which expect finite arrays. To be processed, stored, and transmitted by digital
devices, it needs to be converted into a series of discrete values, known as a digital representation.

If you look at any audio dataset, you'll find digital files with sound excerpts, such as text narration or music.
You may encounter `.wav` (Waveform Audio File), `.flac` (Free Lossless Audio Codec), `.mp3` (MPEG-1 Audio Layer 3), and
many other formats. These formats mainly differ in how they compress the digital representation of the audio signal.

Let's take a look at how we arrive from a continuous signal to this representation. The analog signal is first captured by
a microphone, which converts the sound waves into an electrical signal. The electrical signal is then digitized by an
Analog-to-Digital Converter to get the digital representation through sampling.

## Sampling and sampling rate

Sampling is the process of measuring the value of a continuous audio signal at fixed time steps. The sampled waveform is _discrete_, since it contains a finite number of signal values at uniform intervals. 
a continuous signal into a series of samples. A sample is a value of the signal at a point in time.

![Signal_Sampling.png](../../assets/img/Signal_Sampling.png)
*Illustration from Wikipedia article: [Sampling (signal processing)](https://en.wikipedia.org/wiki/Sampling_(signal_processing))*

The **sampling rate** (also called sampling frequency) is the number of samples taken in one second. The higher the sampling
rate, the more information from the sound wave is obtained. However, it also leads to increase in the computational cost
of processing such files. On the other hand, files with low sampling rates result in higher information loss but are faster and
cheaper to compute with.

The sampling rate is measured in hertz (Hz). To give you a point of reference, CD-quality audio has a sampling rate of 44,100 Hz, meaning
samples are taken 44,100 times per second. For comparison, high-fidelity audio has a sampling rate of 192,000 Hz.
A common sampling rate used in pre-training is 16,000 Hz.

It's important to ensure that all audio examples in your dataset have the same sampling rate when working on any audio task.
If you plan to use custom audio data to fine-tune a pre-trained model, the sampling rate of your data should match the
sampling rate of the data the model was pre-trained on. The sampling rate determines the time interval between successive
audio samples, which impacts the temporal resolution of the audio data. Consider an example: a 5-second sound at a sampling
rate of 16,000 Hz will be represented as a series of 80,000 values, while the same 5-second sound at a sampling rate of
8,000 Hz will be represented as a series of 40,000 values. Transformer models that solve audio tasks treat examples as
sequences and rely on attention mechanisms to learn audio or multimodal representation. Since sequences are different for
audio examples at different sampling rates, it will be challenging for models to generalize between sampling rates.

## Amplitude and bit depth

While the sampling rate tells you how often the samples are taken, what exactly are the values in each sample?

One of the audio wave attributes is amplitude. **Amplitude** is the relative strength of sound waves, which we perceive as
volume. It is measured in decibels (dB), which refer to the sound pressure level or intensity. To give you an example,
a normal speaking voice is under 60 dB, and a rock concert can be at around 125 dB, pushing the limits of human hearing.

To make sure that the digital representation closely approximates the original continuous sound wave, for each audio
sample a number of possible amplitude values is recorded. This number is called **audio bit depth**. The higher the bit depth,
the more amplitude values per sample are captured to recreate the original audio signal.

The most common audio bit depths are 16-bit, 24-bit, and 32-bit. Each is a binary term, representing a number of possible
values. Systems of higher audio bit depths are able to express more possible values:

* 16-bit: 65,536 amplitude values
* 24-bit: 16,777,216 amplitude values
* 32-bit: 4,294,967,296 amplitude values

## Looking at the sound

### Waveform

You may have seen sounds visualized as a waveform. A waveform is an abstract representation of sound waves that illustrates
the changes in a signal's amplitude over time (this change is also often referred to as amplitude envelope).
This type of visualization is useful for identifying specific features of the audio signal such as the timing of individual
sound events, the overall loudness of the signal, and any irregularities or noise present in the audio.

Let's take a look at one. To plot a waveform for an audio example, we can use a Python library called `librosa`:

```bash
pip install librosa
```

Let's take an example sound called "trumpet" that comes with the library:

```py
>>> import librosa

>>> array, sampling_rate = librosa.load(librosa.ex('trumpet'))
```

The example is loaded as a tuple of audio time series (here we call it `array`), and sampling rate (`sampling_rate`).
Let's take a look at this sound's waveform by using librosa's `waveshow()` function:

```py
>>> import matplotlib.pyplot as plt
>>> import librosa.display

>>> plt.figure().set_figwidth(12)
>>> librosa.display.waveshow(array, sr=sampling_rate)
```

![waveform_plot.png](../../assets/img/waveform_plot.png)

The y-axis of the plot shows the amplitude values of the signal while the the x-axis represents the time.
Visualizing audio data along with listening to it can be a useful tool for understanding the data you are working with.
You can see the shape of the signal, observe patterns, learn to spot noise or distortion. If you preprocess data in some
ways, such as normalization, resampling, or filtering, you can visually confirm that preprocessing steps have been applied as expected.
After training a model, you can also visualize samples where errors occur (e.g. in audio classification task) to debug
the issue.

<!--  TODO: in the audio classification section we can add this (visualize errors)-->

### Spectrogram

Another way to visualize audio data is called a **spectrogram**. A spectrogram is a visual representation of the frequency
content of an audio signal as it changes over time. It allows you to see time, frequency, and amplitude all on one graph.

It is one of the most informative audio tools available to you. For example, when working with a music recording, you can
see the various instruments and vocal tracks and how they contribute to the overall sound. In speech, you can identify
different vowel sounds.

Let's plot a spectrogram for the same trumpet sound.The spectrogram is created by taking short segments of the audio signal,
typically lasting a few milliseconds, and calculating the Fourier transform of each segment to obtain its frequency spectrum.

<!--  TODO: @hollance please add an explanation here of what a Fourier transform is.-->

The resulting spectra are then stacked together over time to create the spectrogram.

To do so, we first need to use `librosa.stft()` function to create the short-time Fourier transform before we can plot the spectrogram.
You'll rarely see the result of STFT plotted directly as it can be difficult to interpret. A common practice is to
convert the amplitude spectrogram to a to decibel-scaled spectrogram with `librosa.amplitude_to_db()` function that will
scale the amplitude absolute value scaled relative to `ref`. The decibel scale compresses the dynamic range of the
amplitude spectrogram, making it easier to see the finer details in the spectrogram.

```py
>>> import numpy as np

>>> D = librosa.stft(array)
>>> S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)

>>> plt.figure().set_figwidth(12)
>>> librosa.display.specshow(S_db, x_axis='time')
>>> plt.colorbar()
```

![spectrogram_plot.png](../../assets/img/waveform_plot.png)

In this plot the x-axis represents time, the y-axis represents frequency, and the intensity of the color
represents the amplitude or power of the frequency component at each point in time.

Now that we know what a spectrogram is, let's take a look at a variant of it widely used for signal processing: the mel-spectrogram.

### Mel-spectrogram

A mel-spectrogram is a variation of the spectrogram that is commonly used in audio processing and machine learning tasks.
It is similar to a spectrogram in that it shows the frequency content of an audio signal over time, but it is computed
differently and has a different frequency axis.

In a standard spectrogram, the frequency axis is linear and is measured in hertz (Hz). However, the human auditory system
is more sensitive to changes in lower frequencies than higher frequencies, and this sensitivity decreases logarithmically as
frequency increases. The mel-scale is a perceptual scale that approximates the non-linear frequency response of the human ear.

To create a mel-spectrogram, the audio signal is first divided into short overlapping windows, and a Fourier transform
is applied to each window to obtain the frequency spectrum. Then, the power spectrum according to the mel-scale.

Let's see how we can plot a mel-spectrogram:

```py
>>> S = librosa.feature.melspectrogram(y=array, sr=sampling_rate, n_mels=128, fmax=8000)
>>> S_dB = librosa.power_to_db(S, ref=np.max)

>>> plt.figure().set_figwidth(12)
>>> librosa.display.specshow(S_dB, x_axis='time', y_axis='mel', sr=sampling_rate, fmax=8000)
>>> plt.colorbar()
```

![mel-spectrogram.png](../../assets/img/mel-spectrogram.png)

In the example above, `n_mels` stands for number of mel bands to generate. Mel bands refer to a set of frequency ranges
that are used to divide the spectrum of a sound signal into perceptually meaningful components. Mel bands are typically
defined by a set of triangular filters that are evenly spaced in the mel scale. These filters are designed to respond to
different frequency ranges, and their shape and spacing are chosen to mimic the way the human ear responds to different
frequencies. Common values for `n_mels` are 32, 64, 128. `fmax` indicates the highest frequency (in Hz).

Compared to a standard spectrogram, a mel-spectrogram can capture more meaningful features of the audio signal for human
perception, making it a popular choice in tasks such as speech recognition, speaker identification, and music genre classification.

Now that you know how to visualize audio data examples, go ahead and try to see what your favorite sounds look like :)